%% $Id: bmaur.tex,v 1.17 2017/08/14 18:45:51 jracine Exp jracine $

%\VignetteIndexEntry{bmmaur}
%\VignetteDepends{boot,np,quadprog,tseries}
%\VignetteKeywords{nonparametric, unit root.}
%\VignettePackage{hr}

\documentclass[11pt]{amsart}

\usepackage{setspace}
\usepackage[agsm]{harvard}
\usepackage{pdflscape}

\newtheorem{assumption}{Assumption}[section]
\newtheorem{theorem}{Theorem}[section]

\DeclareMathOperator*{\argmin}{arg\,min}
\tolerance=5000
\allowdisplaybreaks

%% Change the default page sizes.

\setlength{\topmargin}{-0.25in}
\setlength{\textheight}{8.5in}
\setlength{\oddsidemargin}{.0in}
\setlength{\evensidemargin}{.0in}
\setlength{\textwidth}{6.5in}
\setlength{\footskip}{.5in}

\title{Bootstrap Model Averaging Unit Root Inference}
\date{\today}
\author{Bruce E.\ Hansen}
\author{Jeffrey S.\ Racine}

\address[Jeffrey S.\ Racine]{Department of Economics and Graduate
  Program in Statistics, McMaster University, racinej@mcmaster.ca;
  Department of Economics and Finance, La Trobe University;
  Info-Metrics Institute, American University; Rimini Center for
  Economic Analysis; Center for Research in Econometric Analysis of
  TimE Series (CREATES), Aarhus University.}

\thanks{I would like to gratefully acknowledge support from the
  Natural Sciences and Engineering Research Council of Canada
  (NSERC:www.nserc.ca), the Social Sciences and Humanities Research
  Council of Canada (SSHRC:www.sshrc.ca), and the Shared Hierarchical
  Academic Research Computing Network (SHARCNET:www.sharcnet.ca). I
  would like to thank but not implicate James MacKinnon, Bruce Hansen,
  Rob Hyndman and In Choi for their insight and encouragement.}

\pagestyle{plain}

\begin{document}

\begin{abstract}

  Classical unit root tests are known to suffer from potentially
  crippling size distortions, and a range of procedures have been
  proposed to attenuate this problem, including the use of bootstrap
  procedures.  It is also known that the estimating equation's
  functional form can affect the outcome of the test, and various
  model selection procedures have been proposed to overcome this
  limitation. In this paper, we adopt a model averaging procedure to
  deal with model uncertainty at the testing stage. In addition, we
  leverage an automatic model-free dependent bootstrap procedure where
  the null is imposed by simple differencing (the block length is
  automatically determined using recent developments for bootstrapping
  dependent processes). Monte Carlo simulations indicate that this
  approach exhibits the lowest size distortions among its peers in
  settings that confound existing approaches, while it has better
  power than those peers whose size distortions do not preclude their
  general use. The proposed approach is fully automatic, and there are
  no nuisance parameters that have to be set by the user, which ought
  to appeal to practitioners.

\end{abstract}

\maketitle

\centerline{\textit{** Preliminary and Incomplete - Not to be Quoted
    or Circulated Without Permission **}}

\doublespacing

\section{Introduction}

Though unit root tests were developed over four decades ago, problems
with the various approaches that have been proposed persist and,
perhaps surprisingly, there remains room for improvement. When testing
for a unit root, the null is that the series contains a unit root,
with rejection of the null in one direction indicating that a series
is stationary. If these tests exhibit large upwards size distortions,
practitioners may wrongly conclude that a time series is stationary
when in fact it is not, which can render subsequent inference
invalid. Size distortions surface surprisingly often in this setting
since practitioners must select from among a range of candidate
estimating equations when testing for the presence of a unit root, and
estimating equation misspecification leads to bias in estimated
parameters. To deal with the size distortions arising from
misspecification, bootstrap procedures have been proposed
(\citeasnoun{PARK:2003}, \citeasnoun{PALM_ET_AL:2008}). However, for
many of these tests, size distortions and sub-optimal power concerns
persist in part because they rely on a model specification which must
be selected by the practitioner from amount a set of misspecified
candidate models.  In order to attenuate the size distortions arising
from the choice of a misspecified estimating equation, the use of
model selection criteria such as the Bayes Information Criterion (BIC)
\cite{SCHWARZ:1978} has been advocated
\cite{NG_PERRON:2001}. \citeasnoun{NG_PERRON:2001} assert that the BIC
selects overly parsimonious models and propose a Modified Information
Criteria (MIC). For \possessivecite{NG_PERRON:2001} approach, size
distortions are attenuated as the dimension of the selected model
increases, however this is not without cost as power falls as the
dimension increases. Furthermore, the maximum lag that must be set by
the practitioner affects the outcome of the test and ad-hoc rules are
frequently adopted for its selection \cite{SCHWERT:1989}.

As an alternative to model selection, we could instead exploit recent
developments in (frequentist) model averaging (\citeasnoun[Mallows
Model Averaging (MMA)]{HANSEN:2007}, \citeasnoun[Jackknife Model
Averaging (JMA)]{HANSEN_RACINE:2012}, \citeasnoun{HANSEN:2014}), as it
is known that model averaging can overcome limitations associated with
the use of model selection methods. In the present context, we propose
a unit root statistic that is a weighted average of unit root
statistics taken from a set of candidate estimating equations that are
the same as those used for \possessivecite{NG_PERRON:2001} or
\possessivecite{DICKEY_FULLER:1979} approaches. We also adopt an
automatic, model-free, time series bootstrap procedure for
constructing the null distribution of the proposed statistic, from
which nonparametric critical values or nonparametric $P$-values can be
obtained.  Most existing bootstrap unit root procedures are
model-based, and one problem with model-based resampling is that the
data generating process (DGP) is unknown and must be identified from
the series at hand. In order to ensure that the bootstrap samples have
the same structure as the series at hand, this identification must be
correct. \citeasnoun{SWENSEN:2003} considers model-based and
model-free bootstrap approaches and demonstrates that a
difference-based model-free approach (along the lines of that proposed
herein) delivers a bootstrap distribution that approaches the true
asymptotic distribution under the null of a unit root (see also
\citeasnoun[Section 2.4]{PALM_ET_AL:2008}).

We will see that when model averaging is combined with a model-free,
automatic, time series bootstrap procedure
\cite{POLITIS_ROMANO:1994,POLITIS_WHITE:2004,PATTON_POLITIS_WHITE:2009},
we can obtain a fully automatic data-driven procedure that has better
size \textsl{and} power than its peers, while the sensitivity to the
dimension of the model (i.e., the maximum lag that must be set by the
practitioner) is attenuated by averaging over a set of candidate
estimating equations in a particular manner. Furthermore, unlike its
peers, the procedure is very robust to the number of and maximum
dimension of the candidate estimating equations over which the
averaging is performed (size and power are largely unaffected whether
you use augmented Dickey-Fuller models with, e.g., one through two,
four, eight, sixteen, or twenty four differenced lags of the time
series). To the best of our knowledge, ours is the first to consider
model averaging procedures in the unit root setting.

We compare the proposed bootstrap model average approach with the
classic \citeasnoun{DICKEY_FULLER:1979} method,
\possessivecite{PHILLIPS_PERRON:1988} procedure, and with
\possessivecite{NG_PERRON:2001} procedure (using the detrending
approach of \citeasnoun{PERRON_QU:2007}) which is a state-of-the-art
procedure that appears to be the go-to method for most
practitioners. The proposed bootstrap model average approach emerges
as the procedure of choice based upon a fairly extensive Monte Carlo
comparison with the existing go-to and classical approaches.
\possessivecite{NG_PERRON:2001} procedure is based on the same
estimating equations as those \citeasnoun{DICKEY_FULLER:1979}, but
they first detrend the series (this reduces size distortions when
there is a large negative moving average root in the differenced
series) and then use a novel lag selection procedure that chooses a
larger lag length than traditional lag selection procedures.  The
\citeasnoun{DICKEY_FULLER:1979} and \citeasnoun{NG_PERRON:2001} tests
use a parametric autoregression to approximate the ARMA structure of
the errors in the test regression, while
\possessivecite{PHILLIPS_PERRON:1988} procedure instead corrects for
any serial correlation and heteroskedasticity in the errors of the
estimating equation by directly modifying the test statistic. We
direct the reader to the original references for further details.

It has been established that the classical approach
\cite{DICKEY_FULLER:1979} which uses tabulated critical values
sometimes suffers from crippling size
distortions. \possessivecite{MACKINNON:1996} improved tabulated
critical values do not attenuate such size distortions,
unfortunately. \citeasnoun{SCHWERT:1989} conducted extensive Monte
Carlo simulations and demonstrated how there can exist considerable
bias present in a misspecified estimating equation for unit root
testing in the presence of a moving average error process (i.e., in
the presence of a large negative moving average root). In such cases,
the critical values depend on the unknown parameters hence tabulated
Dickey-Fuller critical values should be avoided, and appropriate
bootstrap procedures may be necessary for sound inference in this
setting. \citeasnoun{DEJONG1992323}, again via extensive simulations,
demonstrate that the augmented Dickey-Fuller tests have low power in
the presence of a large autoregressive root. One important practical
aspect for augmented Dickey-Fuller unit root tests is the
specification of the lag length. If it is too small, then the
remaining serial correlation in the errors will bias the test. If it
is too large, then the power of the test will
suffer. \citeasnoun{HANSEN:1995} demonstrates how large power gains
can be achieved by including correlated stationary covariates in the
estimating equation (this could be trivially incorporated in the
proposed bootstrap model average
approach). \citeasnoun{NG_PERRON:2001} point out that a high order
augmented autoregression is often necessary for unit root tests to
have good size, but that information criteria such as the BIC tend to
select a truncation lag that is small, and propose a Modified
Information Criteria (MIC) along with GLS detrended data and
demonstrate how this improves size but can lead to a loss in
power. \citeasnoun{PERRON_QU:2007} propose an improved method for
detrending for the \citeasnoun{NG_PERRON:2001} approach. We direct the
interested reader to \citeasnoun{CHOI:2015} who presents a
state-of-the art treatment of unit root inference.

The rest of this paper proceeds as follows. Section
\ref{sec:background} presents some background on unit root
inference. Section \ref{sec:bmma} outlines the proposed
approach. Section \ref{sec:mc} presents results from a Monte Carlo
simulation that compares the proposed approach with the classic
\citeasnoun{DICKEY_FULLER:1979} procedure, that of
\possessivecite{NG_PERRON:2001}, and a bootstrap BIC model selection
procedure. An R \cite{R} package exists that implements the proposed
method \cite{hr}.

\section{\label{sec:background}Testing for a Unit Root}

Consider a time series $y_{t}$ which satisfies an autoregressive equation
\begin{equation}
y_{t}=\rho y_{t-1}+u_{t}\label{ar}
\end{equation}
for $t=1,\dots,T$, where $u_{t}$ is a stationary I(0) process. This includes
i.i.d.\ processes, white noise, and mean-reverting stationary processes. When
$\rho=1$ then $y_{t}$ is I(1) and we say that $y_{t}$ has a unit root in its
autoregressive representation. On the other hand when $\left\vert
\rho\right\vert <1$ then $y_{t}$ is I(0), is stationary, and does not have a
unit root. Testing $H_{0}\colon\rho=1$ versus $H_{1}\colon\rho<1$ is an important
practical issue in applied time-series modeling.

The most common method for testing the unit root hypothesis is to use the
Augmented Dickey-Fuller (ADF) statistic, which is based on the least-squares
estimation of an autoregressive (AR) model for $y_{t}$. The test can be
described as follows. For some lag order $k$, estimate by least squares the
$k^{th}$ order autoregression
\begin{equation*}
\Delta y_{t}=\widehat{\gamma}(k)y_{t-1}+\sum_{j=1}^{k-1}\widehat{a}
_{j}(k)\Delta y_{t-j}+\widehat{\beta}(k)+\widehat{\epsilon}_{t}(k).
\end{equation*}
Form a $t$-statistic for the null that $\gamma=0$, that is
\begin{equation*}
ADF(k)=\frac{\widehat{\gamma}(k)}{s\left(  \widehat{\gamma}(k)\right)  }
\end{equation*}
where $s\left(  \widehat{\gamma}(k)\right)  $ is a standard error for
$\widehat{\gamma}(k)$. The test rejects for large negative values of $ADF(k)$.
For stationary trend alternatives a linear time trend is also included in the regression.

The conventional asymptotic distribution theory approximates the null
distribution of $ADF(k)$ by either assuming that $k$ is the true
autoregressive order (so that the estimated model is correctly specified) or
by assuming that $k\rightarrow\infty$ as $n\rightarrow\infty$ so that the
model is approximately correct. We take a different approach and derive the
asymptotic distribution without either of these assumptions.

We use the following regularity condition on the fundamental errors $u_{t}$
defined in \eqref{ar}. 

\begin{assumption}
For some $p>r>2\,$, $u_{t}$ is a strictly stationary, zero mean, strong mixing
process of size $-pr/(p-r)$, $E\left\vert u_{i}\right\vert ^{p}<\infty$, and
$\omega^{2}>0$ where
\begin{equation*}
\omega^{2}=\sum_{j=-\infty}^{\infty}E\left(  u_{t}u_{t-j}\right)  .
\end{equation*}

\end{assumption}

Assumption 1 is a mild set of standard mixing conditions which allow
for broad I(0) processes, and encompasses standard AR and ARMA
processes. The assumption that the long-run variance $\omega^{2}$ is
positive excludes over-differenced processes.

Our representation of the asymptotic distribution of the ADF statistic will be
written in terms of the approximating models. For each $k$ define the
approximate model
\begin{equation*}
u_{t}=\sum_{j=1}^{k-1}a_{j}(k)u_{t-j}+\epsilon_{t}(k)
\end{equation*}
by projection. That is, the coefficients $a_{j}(k)$ are defined so that
$E\left(  u_{t-j}\epsilon_{t}(k)\right)  =0$ for $j=1,\dots,k-1$. This defines
the AR(k) approximate model and error. Given the error $\epsilon_{t}(k)$, we
can define its variance, autocovariance and long-run variance
\begin{align}
\sigma^{2}(k) &  =E\left(  \epsilon_{t}(k)^{2}\right)  \label{sigk}\\
\lambda(k) &  =\sum_{j=1}^{\infty}E\left(  \epsilon_{t}(k)\epsilon
_{t-j}(k)\right)  \label{lk}\\
\nu^{2}(k) &  =\sigma^{2}(k)+2\lambda(k).\label{vk}
\end{align}
The parameter $\sigma^{2}(k)$ is the variance of $\epsilon_{t}(k)$,
$\lambda(k)$ is the sum of its autocovariances, and $\nu^{2}(k)$ is its
long-run variance. Under misspecification the error $\epsilon_{t}(k)$ has
serial correlation so that $\lambda(k)\neq0$ and $\sigma^{2}(k)\neq\nu^{2}
(k)$. Under correct specification the error is white noise so $\lambda(k)=0$
and $\sigma^{2}(k)=\nu^{2}(k)$. Thus for small $k$ we expect $\lambda(k)\neq0$
and $\sigma^{2}(k)\neq\nu^{2}(k)$ but for large $k$ we expect $\lambda
(k)\simeq0$ and  $\sigma^{2}(k)\simeq\nu^{2}(k)$, though there is no reason to
expect equality for any finite $k.$

\begin{theorem}\label{thm:thm1}
Under Assumption 1 and $H_{0}\colon\rho=1$, jointly over $k=1,\dots,K$, as
$n\rightarrow\infty$
\begin{equation}
ADF(k)\rightarrow_{d}\frac{\nu(k)}{\sigma(k)}\frac{\int_{0}^{1}W^{\ast}
dW}{\left(  \int_{0}^{1}W^{\ast2}\right)  ^{1/2}}+\frac{\lambda(k)}
{\omega\sigma(k)\left(  \int_{0}^{1}W^{\ast2}\right)  ^{1/2}}\label{adf}
\end{equation}
where $W(r)$ is a standard Brownian motion, and $W^{\ast}(r)=W(r)-\int_{0}
^{1}W(r)dr$ (or a detrended Brownian if a time trend is included).
\end{theorem}

Theorem \ref{thm:thm1} shows that the ADF $t$-statistics converge
jointly to mis-specified versions of the classic Dickey-Fuller
$t$-distribution. The distortions are due to misspecified serial
correlation. When the autoregression is correctly specified so that
the error is white noise, then $\lambda(k)=0$ in which case the
distribution in \eqref{adf} simplifies to the classical
$\int_{0} ^{1}W^{\ast}dW/\left( \int_{0}^{1}W^{\ast2}\right) ^{1/2}$
found by \possessivecite{DICKEY_FULLER:1979}.

Theorem \ref{thm:thm1} also shows that the sequence of $t$-statistics
(for different autoregressive orders) converge jointly, and are all
functions of the same Brownian motion process $W(r)$.

The asymptotic distribution in Theorem \ref{thm:thm1} is generally
unknown as it depends on the unknown parameters $\sigma(k)$ and
$\lambda(k)$. However, the distribution can be approximated by
bootstrap methods since these parameters can be consistently
estimated.

By picking a suitably large autoregressive order $k$ the
distributional distortions can be minimized. Larger values of $k$,
however, reduce the power of unit root tests in finite samples. Thus
it has been viewed as desirable to use an autoregressive order $k$
which is large enough to minimize the size distortions but not so
large as to reduce the power of the test. This requires a
data-dependent rule $\widehat{k}$ for selection of $k$. One popular
method is BIC selection. However, \citeasnoun{NG_PERRON:2001} argued
that this produces a $\widehat{k}$ which is too small to alleviate the
size distortion, and proposed instead a Modified Information Criteria
(MIC) designed for the unit root testing problem.

A data-dependent selection rule $\widehat{k}$ leads to a
data-dependent ADF test $ADF(\widehat{k})$. The appropriate null
distribution for $ADF(\widehat{k})$ is unclear, however, as the use of
a selected lag length invalidates the conventional limit theory unless
used without an ad hoc assumption that $\widehat{k}$ diverges with
$n$. Bootstrap critical values could be used instead though no formal
justification has been provided.

%% Bruce - you had "unless without"... added "unless used without" but
%% you might want to correct for your intention

Instead of selection rules, we propose an averaging statistic. For $k=1,\dots,K$
let $w(k)$ be a set of non-negative weights which sum to one, and set
$w=(w(1),\dots,w(K))$. Then an averaging ADF statistic is
\begin{equation*}
ADF(w)=\sum_{k=1}^{K}w(k)ADF(k).
\end{equation*}


The asymptotic distribution of the averaging ADF statistic can be deduced
directly from Theorem 1.

\begin{theorem}\label{thm:thm2}
As $n\rightarrow\infty$
\begin{equation*}
ADF(w)\rightarrow_{d}\left(  \sum_{k=1}^{K}w(k)\frac{\nu(k)}{\sigma
(k)}\right)  \frac{\int_{0}^{1}W^{\ast}dW}{\left(  \int_{0}^{1}W^{\ast
2}\right)  ^{1/2}}+\left(  \sum_{k=1}^{K}w(k)\frac{\lambda(k)}{\omega
\sigma(k)}\right)  \frac{1}{\left(  \int_{0}^{1}W^{\ast2}\right)  ^{1/2}}.
\end{equation*}

\end{theorem}

Theorem \ref{thm:thm2} provides the asymptotic distribution of the
averaging ADF statistic for fixed weights. Like the distribution in
Theorem \ref{thm:thm1}, it is a distorted version of the classic
Dickey-Fuller $t$-distribution.

While the distribution in Theorem \ref{thm:thm2} is generally unknown,
and dependent on the unknown serial correlation properties of the
series $\Delta y_{t}$, it can be approximated by standard bootstrap
methods since the serial correlation properties can be consistently
estimated.

\begin{proof}[Proof of Theorem \ref{thm:thm1}]

  For simplicity we omit the deterministic components from the
  exposition. By the \citeasnoun{HERRNDORF:1984} functional central
  limit theorem
\begin{align*}
\frac{1}{\sqrt{T}}\sum_{t=1}^{[Tr]}u_{t} &  \Rightarrow\omega W(r)\\
\frac{1}{\sqrt{T}}\sum_{t=1}^{[Tr]}\epsilon_{t}(k) &  \Rightarrow\nu(k)W(r)
\end{align*}
where $W(r)$ is standard Brownian motion. This convergence holds jointly over
both equations and over $k$ since the $\epsilon_{t}(k)$ are linear
transformations of the errors $u_{t}$. Applying Theorem 4.1 of \citeasnoun{HANSEN:1992}
\begin{equation*}
\frac{1}{T}\sum_{t=1}^{T}y_{t-1}\epsilon_{t}(k)\rightarrow_{d}\omega\nu
(k)\int_{0}^{1}WdW+\lambda(k)
\end{equation*}
jointly over $k$.

Let $\widehat{a}(k)=\left(  \widehat{a}_{1}(k),\dots,\widehat{a}_{k-1}
(k)\right)  ^{\prime}$ and $a(k)=\left(  a_{1}(k),\dots,a_{k-1}(k)\right)
^{\prime}$. Define $x_{t}(k)=(y_{t-1},\dots,y_{t-k+1})^{\prime}$, $Q(k)=E\left(
x_{t}(k)x_{t}(k)^{\prime}\right)  $ and
\begin{equation*}
\Omega(k)=\sum_{j=-\infty}^{\infty}E\left(  x_{t}(k)\epsilon_{t}
(k)x_{t-j}(k)^{\prime}\epsilon_{t-j}(k)\right)  .
\end{equation*}


By standard manipulations for the asymptotic theory of integrated processes
\begin{equation*}
\left(
\begin{array}
[c]{c}
T\widehat{\gamma}(k)\\
\sqrt{T}\left(  \widehat{a}(k)-a(k)\right)
\end{array}
\right)  \rightarrow_{d}\left(
\begin{array}
[c]{cc}
\omega^{2}\int_{0}^{1}W^{2} & 0\\
0 & Q(k)
\end{array}
\right)  ^{-1}\left(
\begin{array}
[c]{c}
\omega\nu(k)\int_{0}^{1}WdW+\lambda(k)\\
\xi(k)
\end{array}
\right)
\end{equation*}
where $\xi(k)\sim N(0,\Omega(k))$. This convergence is joint across $k$.

Furthermore, the standard errors satisfy
\begin{equation*}
T\left(  s\left(  \widehat{\gamma}(k)\right)  \right)  ^{2}\rightarrow
_{d}\left(  \omega^{2}\int_{0}^{1}W^{2}\right)  ^{-1}\sigma^{2}(k).
\end{equation*}
Together, we find
\begin{equation*}
ADF(k)\rightarrow_{d}\frac{\omega\nu(k)\int_{0}^{1}WdW+\lambda(k)}{\left(
\omega^{2}\int_{0}^{1}W^{2}\right)  ^{1/2}\sigma(k)}=\frac{\nu(k)}{\sigma
(k)}\frac{\int_{0}^{1}WdW}{\left(  \int_{0}^{1}W^{2}\right)  ^{1/2}}
+\frac{\lambda(k)}{\omega\sigma(k)\left(  \int_{0}^{1}W^{2}\right)  ^{1/2}}
\end{equation*}
as claimed.
\end{proof}

\section{\label{sec:bmma}A Model Averaging Bootstrap Procedure}

\subsection{Model Average Estimators}

The goal in model averaging is to reduce estimation variance while
controlling misspecification bias. The Mallows \cite{MALLOWS:1973}
Criterion for the model average estimator \cite{HANSEN:2007} is
\begin{equation*}
  C_n(w)=w'\hat E'\hat Ew+2\sigma^2K'w,
\end{equation*}
where $\hat E$ is the $T\times M$ matrix with columns containing the
residual vector from the $m$th candidate estimating equation, $K$ the
$M\times 1$ vector of the number of parameters in each model, and
$\sigma^2$ the variance from the largest dimensional
model.\footnote{Note that the residual vectors will be of different
  lengths when the model incorporates lags, so some care must be
  exercised when populating $\hat E$, i.e., the first $k-1$ elements
  from the residual vector for the estimating equation models not
  containing lags must be discarded.} This criterion is used to select
the weight vector $\hat w$, i.e.,
\begin{equation*}
  \hat w = \argmin_w C_n(w).
\end{equation*}
Because $\argmin_w C_n(w)$ has no closed-form solution, the weight
vector is found numerically. The solution involves constrained
minimization subject to non-negativity and summation constraints,
which constitutes a classic quadratic programming problem. This
criterion involves nothing more than computing the residuals for each
candidate estimating equation, obtaining the rank of each candidate
estimating equation, and solving a simple quadratic program. The MMA
$C_n(w)$ criterion provides an estimate of the average squared error
from the model average fit, and has been shown to be asymptotically
optimal in the sense of achieving the lowest possible squared error in
a class of model average estimators. See \citeasnoun{HANSEN:2007} for
further details, \citeasnoun{HANSEN:2014} who explores the use of the
Mallows criterion in a time series autoregression setting and notes
that averaging estimators have reduced risk relative to unconstrained
estimation when the covariates are grouped in sets of four or larger
so that a Stein shrinkage effect holds, and suggests that averaging
estimators be restricted to models in which the regressors have been
grouped in this manner. See also the related work of
\citeasnoun{Hansen2010142} who uses a Mallows criterion for combining
forecasts local to a unit root.

\citeasnoun{HANSEN_RACINE:2012} propose an alternative jackknife model
averaging (JMA) criterion for the model average estimator given by
\begin{equation*}
  CV_{n}(w)=\frac{1}{n}(y-\tilde Xw)'(y-\tilde Xw),
\end{equation*}
where $\tilde X$ is the $T\times M$ matrix with columns containing the
jackknife estimator from the $m$th candidate estimating equation
formed by deleting the $t$ observation when constructing the $t$th
prediction. Like its Mallows counterpart, this involves solving a
quadratic program where we minimize
$(y-\tilde Xw)'(y-\tilde Xw)=y'y+w'\tilde X'\tilde Xw-2y'\tilde Xw$
and the first term is ignorable. In the presence of homoskedastic
errors, JMA and MMA are nearly equivalent, but when the errors are
heteroskedastic, JMA has significantly lower MSE.

To obtain a model average test statistic, we take the $ADF(k)$
statistic from each of the $K$ candidate estimating equations and
average them using the weight vector $\hat w$, and call this averaged
statistic $ADF(w)=\sum_{k=1}^K\hat w(k)ADF(k)$. In order to obtain the
null distribution of this statistic, we use a time series bootstrap
with automatic choice of the expected block length.

\subsection{A Unit Root Model Average Bootstrap Procedure}

We consider a first-difference-based bootstrap procedure for obtaining
the sampling distribution of $ADF(w)$ under the null of a unit root
along the lines of \citeasnoun{SWENSEN:2003}, who proves the
consistency of the standard (non-averaged) test without deterministic
components based on the stationary bootstrap (deterministic components
can be added in the same manner as in \citeasnoun{PSARADAKIS:2001};
see \citeasnoun[page 382]{PALM_ET_AL:2008}). The bootstrap procedure
is as follows:

\begin{enumerate}

\item Take the first difference of the series at hand
  $\epsilon_t=(1-B)y_t$, $t=1,\dots,T$ ($\epsilon_t$ could be, e.g., an
  ARMA process)

\item Apply a time series bootstrap with automated block length choice
  to $\epsilon_t$, $t=1,\dots,T$ ($l$ is the expected block length
  obtained for the geometric bootstrap; see
  \citeasnoun{PATTON_POLITIS_WHITE:2009},
  \citeasnoun{POLITIS_WHITE:2004}, \citeasnoun{POLITIS_ROMANO:1994})

\item Take the cumulative sum of this bootstrap residual
  $\epsilon^*_t$, $t=1,\dots,T$ initializing the sum to the first
  realization of the series, which will generate a bootstrap series
  containing a unit root, i.e.,
  $y^*_t=y_1+\sum_{i=2}^t\epsilon^*_i=y^*_{t-1}+\epsilon^*_t$

\item Next, take the bootstrap $ADF(k)^{*}$ statistics from each of the
  $K$ candidate estimating equations and average them using the weight
  vector for the original series $\hat w$, which delivers a bootstrap
  model average statistic $ADF(w)^{*}$ generated under the null

\item Repeat this process $B$ times, then compute percentiles of the
  $B$ bootstrap statistics $ADF(w)^{*}_{1,K},\dots, ADF(w)^{*}_{B,K}$
  which serve as critical values under the null (e.g., $\alpha/2$ and
  $1-\alpha/2$ where $\alpha$ is the desired level of the test);
  finally, determine whether one will reject or fail to reject at
  level $\alpha$

\end{enumerate}

\section{\label{sec:mc}Monte Carlo Simulation for Bootstrap Unit Root Test}

\subsection{Data Generating Processes (DGPs)}

We consider three non-stationary series containing a unit root (a DGP
considered in \citeasnoun{PALM_ET_AL:2008},
$y_t=y_{t-1}+\epsilon_t-0.8\epsilon_{t-1}$, a random walk, and a
random walk with drift), and two stationary series that do not
contain unit roots (a stationary white noise process,
$y_t=\epsilon_t$, where $\epsilon_t$ is i.i.d.\ $N(0,1)$ and a
stationary ARMA(2,2),
$y_t=0.4y_{t-1}+0.2y_{t-2}+\epsilon_t+0.3\epsilon_{t-1}-0.2\epsilon_{t-2}$).

\begin{align*}
\text{[ARIMA(0,1,1)]}\quad y_t & =y_{t-1}+\epsilon_t+\theta_1\epsilon_{t-1}\\
\text{[Random Walk]}\quad y_t & =y_{t-1}+\epsilon_t\\
\text{[Random Walk/Drift]}\quad y_t & =d + y_{t-1}+\epsilon_t\\
\text{[White Noise]}\quad y_t&=\epsilon_t\\
\text{[ARMA(2,2)]}\quad y_t & = \rho_1y_{t-1}+\rho_2y_{t-2}+\epsilon_t+\theta_1\epsilon_{t-1}+\theta_2\epsilon_{t-2}\\
%\text{[Random Walk/Drift]}\quad y_t & =y_{t-1}+d+\epsilon_t
\end{align*}

\subsection{The Monte Carlo Parameters and Estimating Equations}

We conduct $B=399$ bootstrap replications and $M=10000$ Monte Carlo
replications for sample sizes $T=(100,200,300,400)$. We then conduct
the bootstrap test described above and report the empirical size for
tests with levels $\alpha=0.05$. The underlying DGPs do not contain
deterministic trends, but we consider two versions of the test, one
without and one incorporating a deterministic trend in the estimating
equations.

\begin{enumerate}

\item No deterministic trend - the models used are no constant DF(nc),
  a constant DF(c), and a range of augmented models containing a
  constant and lagged differences of the series (ADF(1), ADF(2),
  without trend etc.).

\item Deterministic trend - the models used are no constant DF(nc), a
  constant DF(c), a constant and time trend DF(ct), and a range of
  augmented models containing a constant and lagged differences of the
  series (ADF(1), ADF(2), with trend etc.).

\end{enumerate}

The R code for this simulation appears in the Appendix.

\subsection{Discussion}

Table \ref{tab:maic_mma_comp} presents a summary of the Monte Carlo
simulations. \citeasnoun[p.\ 344]{PHILLIPS_PERRON:1988} point out a
limitation of their approach writing that their tests ``have
significant size distortions and are too liberal to be useful for
$\theta=-0.5,-0.8$'' (here $\theta=-0.8$ is the MA coefficient used in
the simulations for the ARIMA(0,1,1) DGP). The simulations indeed
reveal that this approach rejects the non-stationary ARIMA(0,1,1) DGP
100\% of the time incorrectly indicating that it is stationary when in
fact it contains a unit root. Furthermore, it is evident that similar
behaviour is manifest in the ADF test as well.  These tests are unable
to distinguish between the non-stationary ARIMA(0,1,1) DGP that
contains a unit root and, e.g., the stationary ARMA(2,2) DGP, though
the Ng-Perron and the proposed test can successfully discriminate
between these two DGPs. Given the inability of the Phillips-Perron and
ADF tests to distinguish between such cases, it would be difficult to
recommend these approaches to practitioners as they may incorrectly
indicate that a series is stationary when in fact it contains a unit
root. Note that for the Ng-Perron approach you also need to specify
whether to include trend, drift, or none (this is not resolved by the
use of their modified information criterion statistic though it could
be performed manually).

\begin{table}[!ht]
\caption{\label{tab:maic_mma_comp}Empirical rejection probabilities
  for testing whether or not a series contains a unit root (nominal
  level $\alpha=0.05$). We compare the Phillips-Perron test (P-P), the
  ADF test based upon BIC model selection (ADF), Ng-Perron's test
  using Perron-Qu detrending (N-P), and the proposed Bootstrap 
  Model Averaging test (MA). The N-P, MA, and ADF-BIC methods use
  the same set of candidate estimating equations determined by
  Schwert's ad-hoc lag selection rule, $k=1,\dots,12(T/100)^{1/4}$. R
  code to replicate can be found in Appendix \ref{app:R}. Results on
  the left are for candidate estimating equations without a
  deterministic trend, on the right with a deterministic trend (the
  DGPs used in the Monte Carlo do not contain deterministic trends,
  the P-P test contains a trend in both tables).}
\input /Users/jracine/tex/categorical/bootstrap_unitroot/work_jma/Schwert_12_no_trend/tex/reject_2f
\quad
\input /Users/jracine/tex/categorical/bootstrap_unitroot/work_jma/Schwert_12_trend/tex/reject_2f
\end{table}

Table \ref{tab:norm_size} presents the normed size distortions,
$||\hat\alpha-0.05||$, for each method as the constant $j$ in
Schwert's ad-hoc formula, $k=1,\dots,j(T/100)^{1/4}$, changes for
$j=2,4,8,12,24$. We take the vector of empirical sizes for the size
DGPs and jointly compute their norm. It is evident that the P-P and
ADF approaches display size distortions that are orders of magnitude
larger than those for the N-P or proposed approach. The size
distortions present in the proposed approach are remarkably robust to
the choice of $j$, unlike that for the N-P approach, and on balance
are the smallest over all approaches and choice of $j$. Table
\ref{tab:norm_power} presents normed power (we take the vector of
empirical powers for the power DGPs and jointly compute their norm
against a base of 1), and it is evident that the proposed method has
size and power that is robust to the number of candidate estimating equations while
the N-P approach is extremely sensitive to its choice. Furthermore,
using Schwert's ad-hoc rule for choosing $j$ may deliver a N-P test
with acceptable size, but it results in a clear loss in power.

\begin{table}[!ht]
  \caption{\label{tab:norm_size}Normed size distortions
    $||\hat\alpha-0.05||$ for the ARIMA(0,1,1), random walk, and random
    walk with drift DGPs over all sample sizes for a range of constants
    $j$ in Schwerts' ad-hoc rule $k=1,\dots,j(T/100)^{1/4}$ (Schwert
    recommends $j=12$), where $\hat\alpha$ is the empirical rejection
    probability. Smaller values indicate empirical size closer to
    nominal size (0.05) overall. Results on the left are for candidate
    estimating equations without a deterministic trend, on the right
    with a deterministic trend (the DGPs used in the Monte Carlo do not
    contain deterministic trends, the P-P test contains a trend in both
    tables)}
\input /Users/jracine/tex/categorical/bootstrap_unitroot/work_jma/tex/norm_size_no_trend_2f
\quad
\input /Users/jracine/tex/categorical/bootstrap_unitroot/work_jma/tex/norm_size_trend_2f
\end{table}

\begin{table}[!ht]
  \caption{\label{tab:norm_power}Normed power $||\hat\alpha-1||$ for the
    ARMA(2,2) and white noise DGPs over all sample sizes for a range of
    constants $j$ in Schwerts' ad-hoc rule $k=1,\dots,j(T/100)^{1/4}$
    (Schwert recommends $j=12$), where $\hat\alpha$ is the empirical
    rejection probability. Smaller values indicate empirical power
    closer to 1 overall. Results on the left are for candidate
    estimating equations without a deterministic trend, on the right
    with a deterministic trend (the DGPs used in the Monte Carlo do not
    contain deterministic trends, the P-P test contains a trend in both
    tables)}
\input /Users/jracine/tex/categorical/bootstrap_unitroot/work_jma/tex/norm_power_no_trend_2f
\quad
\input /Users/jracine/tex/categorical/bootstrap_unitroot/work_jma/tex/norm_power_trend_2f
\end{table}

The choice of tests therefore comes down to either Ng-Perron or the
proposed bootstrap model averaging approach. The proposed approach
displays the smallest size distortions and has higher power tan the
Ng-Perron procedure.  On the basis of our simulation results, we
therefore recommend the MA procedure for testing for the presence of
a unit root with only a handful of candidate estimating equations
being necessary (use, e.g., $j=4$ in Schwert's ad-hoc rule). It
attenuates the large size distortions present in the classical
approach that relies on tabulated critical values and still present
(though muted) in \possessivecite{NG_PERRON:2001} approach, exhibits
higher power, does not require specification of the model from which
the bootstrap resamples are drawn, and uses an automatic block length
selection procedure for the dependent bootstrap method. This procedure
ought to appeal to practitioners as there are no unknown parameters
that must be specified by the user.

Appendix \ref{app:mma} summarizes the MA weights selected by the
proposed procedure for the interested reader.

\section{Conclusion}

We propose a bootstrap model averaging procedure to attenuate known
size distortions that arise when testing for the presence of a unit
root. We adopt a model-free bootstrap procedure where the null is
imposed by simple differencing, and we exploit recent developments in
automatic block length selection for the geometric bootstrap
procedure invoked. We adopt a model averaging procedure, and
simulations indicate that the proposed approach has acceptable size
and higher power than its peers (excluding those that exhibit
unacceptable size distortions such as the Phillips-Perron and ADF
tests). Since there are no nuisance parameters to be set by the user,
we are optimistic that the proposed approach will appeal to
practitioners.

\clearpage

\appendix

\singlespacing

\begin{landscape}
\section{\label{app:mma}Model Average Weight Summary}

Table \ref{tab:mma_no_trend} presents the mean weights accorded to
each of the candidate estimating equations over the $M$ Monte Carlo
replications for ADF models with no trend, Table \ref{tab:mma_trend}
for those with a trend. The use of Schwert's ad-hoc rule
($k=1,\dots,12(T/100)^{1/4}$) results in models having from 1 through
12, 14, 16, and 17 lags being used for the procedures for
T=(100,200,300,400).

\begin{table}[!ht]
\centering
\caption{\label{tab:mma_no_trend}Model Averaging Weights (mean over all
  $M$ Monte Carlo replications, no trend).}
\Tiny
\centerline{$T=100$}
\input /Users/jracine/tex/categorical/bootstrap_unitroot/work_jma/Schwert_12_no_trend/tex/mma_n_100
\centerline{$T=200$}
\input /Users/jracine/tex/categorical/bootstrap_unitroot/work_jma/Schwert_12_no_trend/tex/mma_n_200
\centerline{$T=300$}
\input /Users/jracine/tex/categorical/bootstrap_unitroot/work_jma/Schwert_12_no_trend/tex/mma_n_300
\centerline{$T=400$}
\input /Users/jracine/tex/categorical/bootstrap_unitroot/work_jma/Schwert_12_no_trend/tex/mma_n_400
%\end{table}
\bigskip
%\begin{table}[!ht]
%\centering
\caption{\label{tab:mma_trend}Model Averaging Weights (mean over all
  $M$ Monte Carlo replications, trend).}
\Tiny
\centerline{$T=100$}
\input /Users/jracine/tex/categorical/bootstrap_unitroot/work_jma/Schwert_12_trend/tex/mma_n_100
\centerline{$T=200$}
\input /Users/jracine/tex/categorical/bootstrap_unitroot/work_jma/Schwert_12_trend/tex/mma_n_200
\centerline{$T=300$}
\input /Users/jracine/tex/categorical/bootstrap_unitroot/work_jma/Schwert_12_trend/tex/mma_n_300
\centerline{$T=400$}
\input /Users/jracine/tex/categorical/bootstrap_unitroot/work_jma/Schwert_12_trend/tex/mma_n_400
\end{table}
\end{landscape}

\clearpage

\section{Effect of Changing the Number of Candidate Estimating Equations}

\begin{table}[!ht]
  \caption{Empirical rejection probabilities for testing whether or not
    a series contains a unit root (nominal level $\alpha=0.05$). We
    compare the Phillips-Perron test (P-P), the ADF test based upon BIC
    model selection (ADF), Ng-Perron's test using Perron-Qu detrending
    (N-P), and the proposed Bootstrap Model Averaging test
    (MA). The N-P, MA, and ADF-BIC methods use the same set of
    candidate estimating equations determined by modifying the constant
    in Schwert's ad-hoc lag selection rule, $k=1,\dots,j(T/100)^{1/4}$,
    $j=2,4,8,24$ (clockwise from top left), no deterministic trend in
    any model (for $j=2$, $k=(2,2,3,3)$, $j=4$, $k=(4,5,5,6)$, $j=9$,
    $k=(8,10,11,11)$, $j=24$, $k=(24,29,32,34)$).}
\Tiny
\input /Users/jracine/tex/categorical/bootstrap_unitroot/work_jma/Schwert_2_no_trend/tex/reject_2f
\quad
\input /Users/jracine/tex/categorical/bootstrap_unitroot/work_jma/Schwert_4_no_trend/tex/reject_2f

\bigskip

\input /Users/jracine/tex/categorical/bootstrap_unitroot/work_jma/Schwert_8_no_trend/tex/reject_2f
\quad
\input /Users/jracine/tex/categorical/bootstrap_unitroot/work_jma/Schwert_24_no_trend/tex/reject_2f
\end{table}

\clearpage

\begin{table}[!ht]
\caption{Empirical rejection probabilities for testing whether or not
  a series contains a unit root (nominal level $\alpha=0.05$). We
  compare the Phillips-Perron test (P-P), the ADF test based upon BIC
  model selection (ADF), Ng-Perron's test using Perron-Qu detrending
  (N-P), and the proposed Bootstrap Model Averaging test
  (MA). The N-P, MA, and ADF-BIC methods use the same set of
  candidate estimating equations determined by modifying the constant
  in Schwert's ad-hoc lag selection rule, $k=1,\dots,j(T/100)^{1/4}$,
  $j=2,4,8,24$ (clockwise from top left), deterministic trend
  incorporated in all models (for $j=2$, $k=(2,2,3,3)$, $j=4$,
  $k=(4,5,5,6)$, $j=9$, $k=(8,10,11,11)$, $j=24$, $k=(24,29,32,34)$).}
\Tiny
\input /Users/jracine/tex/categorical/bootstrap_unitroot/work_jma/Schwert_2_trend/tex/reject_2f
\quad
\input /Users/jracine/tex/categorical/bootstrap_unitroot/work_jma/Schwert_4_trend/tex/reject_2f

\bigskip

\input /Users/jracine/tex/categorical/bootstrap_unitroot/work_jma/Schwert_8_trend/tex/reject_2f
\quad
\input /Users/jracine/tex/categorical/bootstrap_unitroot/work_jma/Schwert_24_trend/tex/reject_2f
\end{table}

\clearpage

\section{\label{app:R}R Simulation Code}

The following R code conducts the Monte Carlo simulation. It requires
the packages \texttt{tseries} \cite{tseries}, \texttt{CADFtest}
\cite{CADF}, and the package \texttt{hr} \cite{hr} which contains a
function \texttt{hr.test()} that implements the proposed procedure.

\scriptsize
\begin{verbatim}
require(CADFtest)
require(tseries)
require(hr)

set.seed(42)

n <- scan("num_obs.dat")
M <- scan("num_monte.dat")
DGP <- scan("dgp.dat",character())

system("rm *.out *.bak *~")

## We use five DGPs

dgp.func <- function(n,DGP=c("rw","rwd","wn","arma","ma")) {
    DGP <- match.arg(DGP)
    if(DGP=="rw") {
        ## Random walk (null)
        ts(cumsum(rnorm(n)))
    } else if(DGP=="rwd") {
        ## Random walk with drift (null)
        ts(cumsum(rnorm(n,mean=0.1)))
    } else if(DGP=="wn") {
        ## White noise (alternative)
        ts(rnorm(n))
    } else if(DGP=="arma") {
        ## ARMA(2,2) (alternative)
        arima.sim(n=n,list(ar=c(0.4,0.2),ma=c(0.3,-0.2)))
    } else if(DGP=="ma") {
        ## Unit root with MA error (James MacKinnon, largest alpha
        ## from Palm et al)
        arima.sim(list(order=c(0,1,1),ma=-0.8),n=n)
    }
}

## Monte Carlo begins

p.vec <- numeric()
reject.bmma <- numeric()
reject.pp <- numeric()
reject.maic <- numeric()
reject.bic <- numeric()

for(m in 1:M) {

    x <- dgp.func(n,DGP)

    ## Bootstrap Mallow Model Average, Schwert's ad-hoc rule
    
    max.lag <- round(12*(n/100)^0.25)

    out <- hr.test(x,verbose=FALSE,K.vec=1:max.lag)
    reject.bmma[m] <- out$reject
    write(mean(reject.bmma[1:m]),file="bmma_reject.out")
    write(out$mma.weights,file="mma_weights.out",ncol=length(out$mma.weights),append=TRUE)
    write(out$tau,file="tau.out",append=TRUE)
    write(out$e.block.length,file="expected_block_length.out",append=TRUE)
    write(out$decision,file="decision.out",append=TRUE)
    write(out$reject,file="bmma_reject_01.out",append=TRUE)            
    write(out$quantiles,file="tau_quantiles.out",ncol=length(out$quantiles),append=TRUE)

    ## Phillips-Perron (tseries)

    reject.pp[m] <- ifelse(suppressWarnings(pp.test(x)$p.value)<0.05,1,0)
    write(mean(reject.pp[1:m]),file="pp_reject.out")

    ## Ng-Perron with Perron-Qu detrending, Schwert's ad-hoc rule (CADFtest)

    reject.maic[m] <- ifelse(suppressWarnings(CADFtest(x,max.lag.y=max.lag,criterion="MAIC")$p.value)<0.05,1,0)
    write(mean(reject.maic[1:m]),file="maic_reject.out")

    ## ADF, MacKinnon P-values, BIC model selection, Schwert's ad-hoc rule (CADFtest)
    
    reject.bic[m] <- ifelse(suppressWarnings(CADFtest(x,max.lag.y=max.lag,criterion="BIC")$p.value)<0.05,1,0)
    write(mean(reject.bic[1:m]),file="bic_reject.out")
    
    
}
\end{verbatim}
\normalsize

\clearpage

\singlespacing

\bibliography{bmaur}

\end{document}
