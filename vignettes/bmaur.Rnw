%% $Id: bmaur.tex,v 1.17 2017/08/14 18:45:51 jracine Exp jracine $

%\VignetteIndexEntry{bmmaur}
%\VignetteDepends{boot,np,quadprog,tseries}
%\VignetteKeywords{nonparametric, unit root.}
%\VignettePackage{hr}

\documentclass[11pt]{amsart}

\usepackage{graphicx}
\usepackage{setspace}
\usepackage[agsm]{harvard}
\usepackage{pdflscape}

\newtheorem{assumption}{Assumption}[section]
\newtheorem{theorem}{Theorem}[section]

\DeclareMathOperator*{\argmin}{arg\,min}
\tolerance=5000
\allowdisplaybreaks

%% Change the default page sizes.

\setlength{\topmargin}{-0.25in}
\setlength{\textheight}{8.5in}
\setlength{\oddsidemargin}{.0in}
\setlength{\evensidemargin}{.0in}
\setlength{\textwidth}{6.5in}
\setlength{\footskip}{.5in}

\title{Bootstrap Model Averaging Unit Root Inference}
\date{\today}
\author{Bruce E.\ Hansen}
\author{Jeffrey S.\ Racine}

\address[Jeffrey S.\ Racine]{Department of Economics and Graduate
  Program in Statistics, McMaster University, racinej@mcmaster.ca;
  Department of Economics and Finance, La Trobe University;
  Info-Metrics Institute, American University; Rimini Center for
  Economic Analysis; Center for Research in Econometric Analysis of
  TimE Series (CREATES), Aarhus University.}

\thanks{I would like to gratefully acknowledge support from the
  Natural Sciences and Engineering Research Council of Canada
  (NSERC:www.nserc.ca), the Social Sciences and Humanities Research
  Council of Canada (SSHRC:www.sshrc.ca), and the Shared Hierarchical
  Academic Research Computing Network (SHARCNET:www.sharcnet.ca). I
  would like to thank but not implicate James MacKinnon, Bruce Hansen,
  Rob Hyndman and In Choi for their insight and encouragement.}

\pagestyle{plain}

\begin{document}

\begin{abstract}

  Classical unit root tests are known to suffer from potentially
  crippling size distortions, and a range of procedures have been
  proposed to attenuate this problem, including the use of bootstrap
  procedures.  It is also known that the estimating equation's
  functional form can affect the outcome of the test, and various
  model selection procedures have been proposed to overcome this
  limitation. In this paper, we adopt a model averaging procedure to
  deal with model uncertainty at the testing stage. In addition, we
  leverage an automatic model-free dependent bootstrap procedure where
  the null is imposed by simple differencing (the block length is
  automatically determined using recent developments for bootstrapping
  dependent processes). Monte Carlo simulations indicate that this
  approach exhibits the lowest size distortions among its peers in
  settings that confound existing approaches, while it has superior
  power than those peers whose size distortions do not preclude their
  general use. The proposed approach is fully automatic, and there are
  no nuisance parameters that have to be set by the user, which ought
  to appeal to practitioners.

\end{abstract}

\maketitle

\centerline{\textit{** Preliminary and Incomplete - Not to be Quoted
    or Circulated Without Permission **}}

\doublespacing

\section{Introduction}

Though unit root tests were developed over four decades ago, problems
with the various approaches that have been proposed persist and,
perhaps surprisingly, there remains room for improvement. When testing
for a unit root, the null is that the series contains a unit root,
with rejection of the null in one direction indicating that a series
is stationary. If these tests exhibit large upwards size distortions,
practitioners may wrongly conclude that a time series is stationary
when in fact it is not, which can render subsequent inference
invalid. Size distortions surface surprisingly often in this setting
since practitioners must select from among a range of candidate
estimating equations when testing for the presence of a unit root, and
estimating equation misspecification leads to bias in estimated
parameters. To deal with the size distortions arising from
misspecification, bootstrap procedures have been proposed
(\citeasnoun{PARK:2003}, \citeasnoun{PALM_ET_AL:2008}). However, for
many of these tests, size distortions and sub-optimal power concerns
persist in part because they rely on a model specification which must
be selected by the practitioner from amount a set of misspecified
candidate models.  In order to attenuate the size distortions arising
from the choice of a misspecified estimating equation, the use of
model selection criteria such as the Bayes Information Criterion (BIC)
\cite{SCHWARZ:1978} has been advocated
\cite{NG_PERRON:2001}. \citeasnoun{NG_PERRON:2001} assert that the BIC
selects overly parsimonious models and propose a Modified Information
Criteria (MIC). For \possessivecite{NG_PERRON:2001} approach, size
distortions are attenuated as the dimension of the selected model
increases, however this is not without cost as power falls as the
dimension increases. Furthermore, the maximum lag that must be set by
the practitioner affects the outcome of the test and ad-hoc rules are
frequently adopted for its selection \cite{SCHWERT:1989}.

As an alternative to model selection, we could instead exploit recent
developments in (frequentist) model averaging (\citeasnoun[Mallows
Model Averaging (MMA)]{HANSEN:2007}, \citeasnoun[Jackknife Model
Averaging (JMA)]{HANSEN_RACINE:2012}, \citeasnoun{HANSEN:2014}), as it
is known that model averaging can overcome limitations associated with
the use of model selection methods. In the present context, we propose
a unit root statistic that is a weighted average of unit root
statistics taken from a set of candidate estimating equations that are
the same as those used for \possessivecite{NG_PERRON:2001} or
\possessivecite{DICKEY_FULLER:1979} approaches. We also adopt an
automatic, model-free, time series bootstrap procedure for
constructing the null distribution of the proposed statistic, from
which nonparametric critical values or nonparametric $P$-values can be
obtained.  Most existing bootstrap unit root procedures are
model-based, and one problem with model-based resampling is that the
data generating process (DGP) is unknown and must be identified from
the series at hand. In order to ensure that the bootstrap samples have
the same structure as the series at hand, this identification must be
correct. \citeasnoun{SWENSEN:2003} considers model-based and
model-free bootstrap approaches and demonstrates that a
difference-based model-free approach (along the lines of that proposed
herein) delivers a bootstrap distribution that approaches the true
asymptotic distribution under the null of a unit root (see also
\citeasnoun[Section 2.4]{PALM_ET_AL:2008}).

We will see that when model averaging is combined with a model-free,
automatic, time series bootstrap procedure
\cite{POLITIS_ROMANO:1994,POLITIS_WHITE:2004,PATTON_POLITIS_WHITE:2009},
we can obtain a fully automatic data-driven procedure that has
superior size \textsl{and} power than its peers, while the
sensitivity to the dimension of the model (i.e., the maximum lag that
must be set by the practitioner) is attenuated by averaging over a set
of candidate estimating equations in a particular manner. Furthermore,
unlike its peers, the procedure is very robust to the number of and
maximum dimension of the candidate estimating equations over which the
averaging is performed (size and power are largely unaffected whether
you use augmented Dickey-Fuller models with, e.g., one through two,
four, eight, sixteen, or twenty four differenced lags of the time
series). To the best of our knowledge, ours is the first to consider
model averaging procedures in the unit root setting.

We compare the proposed bootstrap model average approach with the
classic \citeasnoun{DICKEY_FULLER:1979} method,
\possessivecite{PHILLIPS_PERRON:1988} procedure, and with
\possessivecite{NG_PERRON:2001} procedure (using the detrending
approach of \citeasnoun{PERRON_QU:2007}) which is a state-of-the-art
procedure that appears to be the go-to method for most
practitioners. The proposed bootstrap model average approach emerges
as the procedure of choice based upon a fairly extensive Monte Carlo
comparison with the existing go-to and classical approaches.
\possessivecite{NG_PERRON:2001} procedure is based on the same
estimating equations as those \citeasnoun{DICKEY_FULLER:1979}, but
they first detrend the series (this reduces size distortions when
there is a large negative moving average root in the differenced
series) and then use a novel lag selection procedure that chooses a
larger lag length than traditional lag selection procedures.  The
\citeasnoun{DICKEY_FULLER:1979} and \citeasnoun{NG_PERRON:2001} tests
use a parametric autoregression to approximate the ARMA structure of
the errors in the test regression, while
\possessivecite{PHILLIPS_PERRON:1988} procedure instead corrects for
any serial correlation and heteroskedasticity in the errors of the
estimating equation by directly modifying the test statistic. We
direct the reader to the original references for further details.

It has been established that the classical approach
\cite{DICKEY_FULLER:1979} which uses tabulated critical values
sometimes suffers from crippling size
distortions. \possessivecite{MACKINNON:1996} improved tabulated
critical values do not attenuate such size distortions,
unfortunately. \citeasnoun{SCHWERT:1989} conducted extensive Monte
Carlo simulations and demonstrated how there can exist considerable
bias present in a misspecified estimating equation for unit root
testing in the presence of a moving average error process (i.e., in
the presence of a large negative moving average root). In such cases,
the critical values depend on the unknown parameters hence tabulated
Dickey-Fuller critical values should be avoided, and appropriate
bootstrap procedures may be necessary for sound inference in this
setting. \citeasnoun{DEJONG1992323}, again via extensive simulations,
demonstrate that the augmented Dickey-Fuller tests have low power in
the presence of a large autoregressive root. One important practical
aspect for augmented Dickey-Fuller unit root tests is the
specification of the lag length. If it is too small, then the
remaining serial correlation in the errors will bias the test. If it
is too large, then the power of the test will
suffer. \citeasnoun{HANSEN:1995} demonstrates how large power gains
can be achieved by including correlated stationary covariates in the
estimating equation (this could be trivially incorporated in the
proposed bootstrap model average
approach). \citeasnoun{NG_PERRON:2001} point out that a high order
augmented autoregression is often necessary for unit root tests to
have good size, but that information criteria such as the BIC tend to
select a truncation lag that is small, and propose a Modified
Information Criteria (MIC) along with GLS detrended data and
demonstrate how this improves size but can lead to a loss in
power. \citeasnoun{PERRON_QU:2007} propose an improved method for
detrending for the \citeasnoun{NG_PERRON:2001} approach. We direct the
interested reader to \citeasnoun{CHOI:2015} who presents a
state-of-the art treatment of unit root inference.

The rest of this paper proceeds as follows. Section
\ref{sec:background} presents some background on unit root
inference. Section \ref{sec:bmma} outlines the proposed
approach. Section \ref{sec:mc} presents results from a Monte Carlo
simulation that compares the proposed approach with the classic
\citeasnoun{DICKEY_FULLER:1979} procedure, that of
\possessivecite{NG_PERRON:2001}, and a bootstrap BIC model selection
procedure. An R \cite{R} package exists that implements the proposed
method \cite{hr}.

\section{\label{sec:background}Testing for a Unit Root}

Consider a time series $y_{t}$ which satisfies an autoregressive equation
\begin{equation}
y_{t}=\rho y_{t-1}+u_{t}\label{ar}
\end{equation}
for $t=1,\dots,T$, where $u_{t}$ is a stationary I(0) process. This includes
i.i.d.\ processes, white noise, and mean-reverting stationary processes. When
$\rho=1$ then $y_{t}$ is I(1) and we say that $y_{t}$ has a unit root in its
autoregressive representation. On the other hand when $\left\vert
\rho\right\vert <1$ then $y_{t}$ is I(0), is stationary, and does not have a
unit root. Testing $H_{0}\colon\rho=1$ versus $H_{1}\colon\rho<1$ is an important
practical issue in applied time-series modeling.

The most common method for testing the unit root hypothesis is to use the
Augmented Dickey-Fuller (ADF) statistic, which is based on the least-squares
estimation of an autoregressive (AR) model for $y_{t}$. The test can be
described as follows. For some lag order $k$, estimate by least squares the
$k^{th}$ order autoregression
\begin{equation*}
\Delta y_{t}=\widehat{\gamma}(k)y_{t-1}+\sum_{j=1}^{k-1}\widehat{a}
_{j}(k)\Delta y_{t-j}+\widehat{\beta}(k)+\widehat{\epsilon}_{t}(k).
\end{equation*}
Form a $t$-statistic for the null that $\gamma=0$, that is
\begin{equation*}
ADF(k)=\frac{\widehat{\gamma}(k)}{s\left(  \widehat{\gamma}(k)\right)  }
\end{equation*}
where $s\left(  \widehat{\gamma}(k)\right)  $ is a standard error for
$\widehat{\gamma}(k)$. The test rejects for large negative values of $ADF(k)$.
For stationary trend alternatives a linear time trend is also included in the regression.

The conventional asymptotic distribution theory approximates the null
distribution of $ADF(k)$ by either assuming that $k$ is the true
autoregressive order (so that the estimated model is correctly specified) or
by assuming that $k\rightarrow\infty$ as $n\rightarrow\infty$ so that the
model is approximately correct. We take a different approach and derive the
asymptotic distribution without either of these assumptions.

We use the following regularity condition on the fundamental errors $u_{t}$
defined in \eqref{ar}. 

\begin{assumption}
For some $p>r>2\,$, $u_{t}$ is a strictly stationary, zero mean, strong mixing
process of size $-pr/(p-r)$, $E\left\vert u_{i}\right\vert ^{p}<\infty$, and
$\omega^{2}>0$ where
\begin{equation*}
\omega^{2}=\sum_{j=-\infty}^{\infty}E\left(  u_{t}u_{t-j}\right)  .
\end{equation*}

\end{assumption}

Assumption 1 is a mild set of standard mixing conditions which allow
for broad I(0) processes, and encompasses standard AR and ARMA
processes. The assumption that the long-run variance $\omega^{2}$ is
positive excludes over-differenced processes.

Our representation of the asymptotic distribution of the ADF statistic will be
written in terms of the approximating models. For each $k$ define the
approximate model
\begin{equation*}
u_{t}=\sum_{j=1}^{k-1}a_{j}(k)u_{t-j}+\epsilon_{t}(k)
\end{equation*}
by projection. That is, the coefficients $a_{j}(k)$ are defined so that
$E\left(  u_{t-j}\epsilon_{t}(k)\right)  =0$ for $j=1,\dots,k-1$. This defines
the AR(k) approximate model and error. Given the error $\epsilon_{t}(k)$, we
can define its variance, autocovariance and long-run variance
\begin{align}
\sigma^{2}(k) &  =E\left(  \epsilon_{t}(k)^{2}\right)  \label{sigk}\\
\lambda(k) &  =\sum_{j=1}^{\infty}E\left(  \epsilon_{t}(k)\epsilon
_{t-j}(k)\right)  \label{lk}\\
\nu^{2}(k) &  =\sigma^{2}(k)+2\lambda(k).\label{vk}
\end{align}
The parameter $\sigma^{2}(k)$ is the variance of $\epsilon_{t}(k)$,
$\lambda(k)$ is the sum of its autocovariances, and $\nu^{2}(k)$ is its
long-run variance. Under misspecification the error $\epsilon_{t}(k)$ has
serial correlation so that $\lambda(k)\neq0$ and $\sigma^{2}(k)\neq\nu^{2}
(k)$. Under correct specification the error is white noise so $\lambda(k)=0$
and $\sigma^{2}(k)=\nu^{2}(k)$. Thus for small $k$ we expect $\lambda(k)\neq0$
and $\sigma^{2}(k)\neq\nu^{2}(k)$ but for large $k$ we expect $\lambda
(k)\simeq0$ and  $\sigma^{2}(k)\simeq\nu^{2}(k)$, though there is no reason to
expect equality for any finite $k.$

\begin{theorem}\label{thm:thm1}
Under Assumption 1 and $H_{0}\colon\rho=1$, jointly over $k=1,\dots,K$, as
$n\rightarrow\infty$
\begin{equation}
ADF(k)\rightarrow_{d}\frac{\nu(k)}{\sigma(k)}\frac{\int_{0}^{1}W^{\ast}
dW}{\left(  \int_{0}^{1}W^{\ast2}\right)  ^{1/2}}+\frac{\lambda(k)}
{\omega\sigma(k)\left(  \int_{0}^{1}W^{\ast2}\right)  ^{1/2}}\label{adf}
\end{equation}
where $W(r)$ is a standard Brownian motion, and $W^{\ast}(r)=W(r)-\int_{0}
^{1}W(r)dr$ (or a detrended Brownian if a time trend is included).
\end{theorem}

Theorem \ref{thm:thm1} shows that the ADF $t$-statistics converge
jointly to mis-specified versions of the classic Dickey-Fuller
$t$-distribution. The distortions are due to misspecified serial
correlation. When the autoregression is correctly specified so that
the error is white noise, then $\lambda(k)=0$ in which case the
distribution in \eqref{adf} simplifies to the classical
$\int_{0} ^{1}W^{\ast}dW/\left( \int_{0}^{1}W^{\ast2}\right) ^{1/2}$
found by \possessivecite{DICKEY_FULLER:1979}.

Theorem \ref{thm:thm1} also shows that the sequence of $t$-statistics
(for different autoregressive orders) converge jointly, and are all
functions of the same Brownian motion process $W(r)$.

The asymptotic distribution in Theorem \ref{thm:thm1} is generally
unknown as it depends on the unknown parameters $\sigma(k)$ and
$\lambda(k)$. However, the distribution can be approximated by
bootstrap methods since these parameters can be consistently
estimated.

By picking a suitably large autoregressive order $k$ the
distributional distortions can be minimized. Larger values of $k$,
however, reduce the power of unit root tests in finite samples. Thus
it has been viewed as desirable to use an autoregressive order $k$
which is large enough to minimize the size distortions but not so
large as to reduce the power of the test. This requires a
data-dependent rule $\widehat{k}$ for selection of $k$. One popular
method is BIC selection. However, \citeasnoun{NG_PERRON:2001} argued
that this produces a $\widehat{k}$ which is too small to alleviate the
size distortion, and proposed instead a Modified Information Criteria
(MIC) designed for the unit root testing problem.

A data-dependent selection rule $\widehat{k}$ leads to a
data-dependent ADF test $ADF(\widehat{k})$. The appropriate null
distribution for $ADF(\widehat{k})$ is unclear, however, as the use of
a selected lag length invalidates the conventional limit theory unless
used without an ad hoc assumption that $\widehat{k}$ diverges with
$n$. Bootstrap critical values could be used instead though no formal
justification has been provided.

%% Bruce - you had "unless without"... added "unless used without" but
%% you might want to correct for your intention

Instead of selection rules, we propose an averaging statistic. For $k=1,\dots,K$
let $w(k)$ be a set of non-negative weights which sum to one, and set
$w=(w(1),\dots,w(K))$. Then an averaging ADF statistic is
\begin{equation*}
ADF(w)=\sum_{k=1}^{K}w(k)ADF(k).
\end{equation*}


The asymptotic distribution of the averaging ADF statistic can be deduced
directly from Theorem 1.

\begin{theorem}\label{thm:thm2}
As $n\rightarrow\infty$
\begin{equation*}
ADF(w)\rightarrow_{d}\left(  \sum_{k=1}^{K}w(k)\frac{\nu(k)}{\sigma
(k)}\right)  \frac{\int_{0}^{1}W^{\ast}dW}{\left(  \int_{0}^{1}W^{\ast
2}\right)  ^{1/2}}+\left(  \sum_{k=1}^{K}w(k)\frac{\lambda(k)}{\omega
\sigma(k)}\right)  \frac{1}{\left(  \int_{0}^{1}W^{\ast2}\right)  ^{1/2}}.
\end{equation*}

\end{theorem}

Theorem \ref{thm:thm2} provides the asymptotic distribution of the
averaging ADF statistic for fixed weights. Like the distribution in
Theorem \ref{thm:thm1}, it is a distorted version of the classic
Dickey-Fuller $t$-distribution.

While the distribution in Theorem \ref{thm:thm2} is generally unknown,
and dependent on the unknown serial correlation properties of the
series $\Delta y_{t}$, it can be approximated by standard bootstrap
methods since the serial correlation properties can be consistently
estimated.

\begin{proof}[Proof of Theorem \ref{thm:thm1}]

  For simplicity we omit the deterministic components from the
  exposition. By the \citeasnoun{HERRNDORF:1984} functional central
  limit theorem
\begin{align*}
\frac{1}{\sqrt{T}}\sum_{t=1}^{[Tr]}u_{t} &  \Rightarrow\omega W(r)\\
\frac{1}{\sqrt{T}}\sum_{t=1}^{[Tr]}\epsilon_{t}(k) &  \Rightarrow\nu(k)W(r)
\end{align*}
where $W(r)$ is standard Brownian motion. This convergence holds jointly over
both equations and over $k$ since the $\epsilon_{t}(k)$ are linear
transformations of the errors $u_{t}$. Applying Theorem 4.1 of \citeasnoun{HANSEN:1992}
\begin{equation*}
\frac{1}{T}\sum_{t=1}^{T}y_{t-1}\epsilon_{t}(k)\rightarrow_{d}\omega\nu
(k)\int_{0}^{1}WdW+\lambda(k)
\end{equation*}
jointly over $k$.

Let $\widehat{a}(k)=\left(  \widehat{a}_{1}(k),\dots,\widehat{a}_{k-1}
(k)\right)  ^{\prime}$ and $a(k)=\left(  a_{1}(k),\dots,a_{k-1}(k)\right)
^{\prime}$. Define $x_{t}(k)=(y_{t-1},\dots,y_{t-k+1})^{\prime}$, $Q(k)=E\left(
x_{t}(k)x_{t}(k)^{\prime}\right)  $ and
\begin{equation*}
\Omega(k)=\sum_{j=-\infty}^{\infty}E\left(  x_{t}(k)\epsilon_{t}
(k)x_{t-j}(k)^{\prime}\epsilon_{t-j}(k)\right)  .
\end{equation*}


By standard manipulations for the asymptotic theory of integrated processes
\begin{equation*}
\left(
\begin{array}
[c]{c}
T\widehat{\gamma}(k)\\
\sqrt{T}\left(  \widehat{a}(k)-a(k)\right)
\end{array}
\right)  \rightarrow_{d}\left(
\begin{array}
[c]{cc}
\omega^{2}\int_{0}^{1}W^{2} & 0\\
0 & Q(k)
\end{array}
\right)  ^{-1}\left(
\begin{array}
[c]{c}
\omega\nu(k)\int_{0}^{1}WdW+\lambda(k)\\
\xi(k)
\end{array}
\right)
\end{equation*}
where $\xi(k)\sim N(0,\Omega(k))$. This convergence is joint across $k$.

Furthermore, the standard errors satisfy
\begin{equation*}
T\left(  s\left(  \widehat{\gamma}(k)\right)  \right)  ^{2}\rightarrow
_{d}\left(  \omega^{2}\int_{0}^{1}W^{2}\right)  ^{-1}\sigma^{2}(k).
\end{equation*}
Together, we find
\begin{equation*}
ADF(k)\rightarrow_{d}\frac{\omega\nu(k)\int_{0}^{1}WdW+\lambda(k)}{\left(
\omega^{2}\int_{0}^{1}W^{2}\right)  ^{1/2}\sigma(k)}=\frac{\nu(k)}{\sigma
(k)}\frac{\int_{0}^{1}WdW}{\left(  \int_{0}^{1}W^{2}\right)  ^{1/2}}
+\frac{\lambda(k)}{\omega\sigma(k)\left(  \int_{0}^{1}W^{2}\right)  ^{1/2}}
\end{equation*}
as claimed.
\end{proof}

\section{\label{sec:bmma}A Model Averaging Bootstrap Procedure}

\subsection{Model Average Estimators}

The goal in model averaging is to reduce estimation variance while
controlling misspecification bias. The Mallows \cite{MALLOWS:1973}
Criterion for the model average estimator \cite{HANSEN:2007} is
\begin{equation*}
  C_n(w)=w'\hat E'\hat Ew+2\sigma^2K'w,
\end{equation*}
where $\hat E$ is the $T\times M$ matrix with columns containing the
residual vector from the $m$th candidate estimating equation, $K$ the
$M\times 1$ vector of the number of parameters in each model, and
$\sigma^2$ the variance from the largest dimensional
model.\footnote{Note that the residual vectors will be of different
  lengths when the model incorporates lags, so some care must be
  exercised when populating $\hat E$, i.e., the first $k-1$ elements
  from the residual vector for the estimating equation models not
  containing lags must be discarded.} This criterion is used to select
the weight vector $\hat w$, i.e.,
\begin{equation*}
  \hat w = \argmin_w C_n(w).
\end{equation*}
Because $\argmin_w C_n(w)$ has no closed-form solution, the weight
vector is found numerically. The solution involves constrained
minimization subject to non-negativity and summation constraints,
which constitutes a classic quadratic programming problem. This
criterion involves nothing more than computing the residuals for each
candidate estimating equation, obtaining the rank of each candidate
estimating equation, and solving a simple quadratic program. The MMA
$C_n(w)$ criterion provides an estimate of the average squared error
from the model average fit, and has been shown to be asymptotically
optimal in the sense of achieving the lowest possible squared error in
a class of model average estimators. See \citeasnoun{HANSEN:2007} for
further details, \citeasnoun{HANSEN:2014} who explores the use of the
Mallows criterion in a time series autoregression setting and notes
that averaging estimators have reduced risk relative to unconstrained
estimation when the covariates are grouped in sets of four or larger
so that a Stein shrinkage effect holds, and suggests that averaging
estimators be restricted to models in which the regressors have been
grouped in this manner. See also the related work of
\citeasnoun{Hansen2010142} who uses a Mallows criterion for combining
forecasts local to a unit root.

\citeasnoun{HANSEN_RACINE:2012} propose an alternative jackknife model
averaging (JMA) criterion for the model average estimator given by
\begin{equation*}
  CV_{n}(w)=\frac{1}{n}(y-\tilde Xw)'(y-\tilde Xw),
\end{equation*}
where $\tilde X$ is the $T\times M$ matrix with columns containing the
jackknife estimator from the $m$th candidate estimating equation
formed by deleting the $t$ observation when constructing the $t$th
prediction. Like its Mallows counterpart, this involves solving a
quadratic program where we minimize
$(y-\tilde Xw)'(y-\tilde Xw)=y'y+w'\tilde X'\tilde Xw-2y'\tilde Xw$
and the first term is ignorable. In the presence of homoskedastic
errors, JMA and MMA are nearly equivalent, but when the errors are
heteroskedastic, JMA has significantly lower MSE.

% A series of
% exploratory simulations reveal that the JMA approach exhibits superior
% size and power over the MMA approach in this setting, so we report
% simulations for the JMA method (those for the MMA method are available
% upon request).

To obtain a model average test statistic, we take the $ADF(k)$
statistic from each of the $K$ candidate estimating equations and
average them using the weight vector $\hat w$, and call this averaged
statistic $ADF(w)=\sum_{k=1}^K\hat w(k)ADF(k)$. In order to obtain the
null distribution of this statistic, we use a time series bootstrap
with automatic choice of the expected block length.

\subsection{A Unit Root Model Average Bootstrap Procedure}

We consider a first-difference-based bootstrap procedure for obtaining
the sampling distribution of $ADF(w)$ under the null of a unit root
along the lines of \citeasnoun{SWENSEN:2003}, who proves the
consistency of the standard (non-averaged) test without deterministic
components based on the stationary bootstrap (deterministic components
can be added in the same manner as in \citeasnoun{PSARADAKIS:2001};
see \citeasnoun[page 382]{PALM_ET_AL:2008}). The bootstrap procedure
is as follows:

\begin{enumerate}

\item Take the first difference of the series at hand
  $\epsilon_t=\Delta y_t$, $t=1,\dots,T$ ($\epsilon_t$ could be, e.g., an
  ARMA process)

\item Apply a time series bootstrap with automated block length choice
  to $\epsilon_t$, $t=1,\dots,T$ ($l$ is the expected block length
  obtained for the geometric bootstrap; see
  \citeasnoun{PATTON_POLITIS_WHITE:2009},
  \citeasnoun{POLITIS_WHITE:2004}, \citeasnoun{POLITIS_ROMANO:1994})

\item Take the cumulative sum of this bootstrap residual
  $\epsilon^*_t$, $t=1,\dots,T$ initializing the sum to the first
  realization of the series, which will generate a bootstrap series
  containing a unit root, i.e.,
  $y^*_t=y_1+\sum_{i=2}^t\epsilon^*_i=y^*_{t-1}+\epsilon^*_t$

\item Next, take the bootstrap $ADF(k)^{*}$ statistics from each of the
  $K$ candidate estimating equations and average them using the weight
  vector for the original series $\hat w$, which delivers a bootstrap
  model average statistic $ADF(w)^{*}$ generated under the null

\item Repeat this process $B$ times, then compute percentiles of the
  $B$ bootstrap statistics $ADF(w)^{*}_{1,K},\dots, ADF(w)^{*}_{B,K}$
  which serve as critical values under the null (e.g., $\alpha/2$ and
  $1-\alpha/2$ where $\alpha$ is the desired level of the test);
  finally, determine whether one will reject or fail to reject at
  level $\alpha$

\end{enumerate}

\section{\label{sec:mc}Monte Carlo Simulation for Bootstrap Unit Root Test}

We construct power curves for three DGPs, one a simple AR(1), one an
ARMA(1,1), and one an ARMA(1,2). The ARMA(1,1) DGP was used in
simulations appearing in \citeasnoun{PALM_ET_AL:2008}
($y_t=\rho y_{t-1}+\epsilon_t-0.8\epsilon_{t-1}$) and is known to
confound existing tests. In particular, \citeasnoun[p.\
344]{PHILLIPS_PERRON:1988} point out a limitation of their approach
writing that their tests ``have significant size distortions and are
too liberal to be useful for $\theta=-0.5,-0.8$'' (here $\theta=-0.8$
is the MA coefficient in the ARMA(1,1) model). For the ARMA(1,2) model
we use the DGP
$y_t=\rho y_{t-1}+\epsilon_t+0.3\epsilon_{t-1}-0.2\epsilon_{t-1}$,
while for the AR(1) DGP we use $y_t=\rho y_{t-1}+\epsilon_t$. 

We conduct $B=399$ bootstrap replications and $M=2500$ Monte Carlo
replications for sample sizes $T=(50,100,200,400)$. We then conduct
the bootstrap test described above and report the empirical rejection
frequency of each test where each test is conducted with a nominal
level of $\alpha=0.05$. When $\rho=1$ we can assess each test's
empirical size while when $\rho<1$ we can assess power, and we
consider $\rho\in[0.75,1]$ using a grid of 15 equally spaced values
for $\rho$ for constructing each power curve. Results for each DGP and
a brief discussion follow.

\subsection{AR(1) DGP}

Figure \ref{fig:arpower} presents power curves under the AR(1) DGP for
the proposed test using either the MMA or JMA weight selection scheme
outlined in Section \ref{sec:bmma}.

\begin{figure}[!ht]
  \includegraphics[scale=0.375]{work_power_ar/n_50/power_all.pdf}
  \includegraphics[scale=0.375]{work_power_ar/n_100/power_all.pdf}

  \includegraphics[scale=0.375]{work_power_ar/n_200/power_all.pdf}
  \includegraphics[scale=0.375]{work_power_ar/n_400/power_all.pdf}
  \caption{\label{fig:arpower}Power curves for the AR(1) DGP
    $y_t=\rho y_{t-1}+\varepsilon_t$ for unit root tests using a
    $\alpha=0.05$ level of significance. When $\rho=1$ a unit root is
    present (empirical size is the height of the power curve at
    $\rho=1$, i.e., at the right of each figure).}
\end{figure}

Figure \ref{fig:arpower} reveals that this is a textbook case where
all tests are correctly sized, though it is evident that the proposed
approach dominates in terms of power. As expected, for a given value
of $\rho$ ($<1$), power increases as the sample size increases for all
tests. \possessivecite{HANSEN:2007} MMA weight selection scheme
delivers a test with more power than that based on
\possessivecite{HANSEN_RACINE:2012} JMA approach for this DGP.

\subsection{ARMA(1,1) DGP}

Figure \ref{fig:mapower} presents power curves under the ARMA(1,1)
DGP for the proposed test using either the MMA or JMA weight selection
scheme outlined in Section \ref{sec:bmma}.

\begin{figure}[!ht]
  \includegraphics[scale=0.375]{work_power_ma/n_50/power_all.pdf}
  \includegraphics[scale=0.375]{work_power_ma/n_100/power_all.pdf}

  \includegraphics[scale=0.375]{work_power_ma/n_200/power_all.pdf}
  \includegraphics[scale=0.375]{work_power_ma/n_400/power_all.pdf}
  \caption{\label{fig:mapower}Power curves for the ARMA(1,1) DGP
    $y_t=\rho y_{t-1}+\varepsilon_t-0.8\varepsilon_{t-1}$ for unit
    root tests using a $\alpha=0.05$ level of significance. When
    $\rho=1$ a unit root is present (empirical size is the height of
    the power curve at $\rho=1$, i.e., at the right of each figure).}
\end{figure}

It is evident from Figure \ref{fig:mapower} that the
\citeasnoun{PHILLIPS_PERRON:1988} and \citeasnoun{DICKEY_FULLER:1979}
approaches completely fail for this DGP as was pointed out by
\citeasnoun{PHILLIPS_PERRON:1988} and \citeasnoun{PALM_ET_AL:2008}. In
particular, the \citeasnoun{PHILLIPS_PERRON:1988} test has empirical
size equal to $1.00$ when $\alpha=0.05$ thereby rejecting 100\% of the
time when in fact the null is true, while the
\citeasnoun{DICKEY_FULLER:1979} approach displays similarly crippling
size distortions.

In light of the extreme size distortions that surface for this (and
similar) DGPs when using the \citeasnoun{PHILLIPS_PERRON:1988} and
\citeasnoun{DICKEY_FULLER:1979} tests, it is difficult to recommend
either test to practitioners, therefore the choice of tests therefore
comes down to either \possessivecite{NG_PERRON:2001} procedure or the
proposed bootstrap model averaging approach. The proposed approach is
correctly sized for this DGP while \possessivecite{NG_PERRON:2001}
approach exhibits large size distortions for samples of size
$n<200$. In addition, the proposed approach has substantially higher
power than \possessivecite{NG_PERRON:2001} approach for this DGP
regardless of whether the MMA or JMA weighting schemes are adopted.

\subsection{ARMA(1,2) DGP}

Figure \ref{fig:armapower} presents power curves under the ARMA(1,2)
for the proposed test using either the MMA or JMA weight selection
scheme outlined in Section \ref{sec:bmma}. Given that neither the
\citeasnoun{PHILLIPS_PERRON:1988} nor \citeasnoun{DICKEY_FULLER:1979}
can be recommended to practitioners given their failure for the
ARMA(1,1) DGP above and for a range of similar DGPs, we drop them in
the power analysis that follows.

\begin{figure}[!ht]
  \includegraphics[scale=0.375]{work_power_arma/n_50/power.pdf}
  \includegraphics[scale=0.375]{work_power_arma/n_100/power.pdf}

  \includegraphics[scale=0.375]{work_power_arma/n_200/power.pdf}
  \includegraphics[scale=0.375]{work_power_arma/n_400/power.pdf}
  \caption{\label{fig:armapower}Power curves for the DGP
    $y_t=\rho
    y_{t-1}+\varepsilon_t+0.3\varepsilon_{t-1}-0.2\varepsilon_{t-2}$
    for unit root tests using a $\alpha=0.05$ level of
    significance. When $\rho=1$ a unit root is present (empirical size
    is the height of the power curve at $\rho=1$, i.e., at the right
    of each figure).}
\end{figure}

\subsection{Discussion}

On the basis of our simulation results, we feel confident recommending
the proposed procedure for testing for the presence of a unit root
with only a handful of candidate estimating equations being necessary
(use, e.g., $j=4$ in Schwert's ad-hoc rule). It attenuates the large
size distortions present in the classical approach that relies on
tabulated critical values and still present (though muted) in
\possessivecite{NG_PERRON:2001} approach, exhibits higher power, does
not require specification of the model from which the bootstrap
resamples are drawn, and uses an automatic block length selection
procedure for the dependent bootstrap method. This procedure ought to
appeal to practitioners as there are no unknown parameters that must
be specified by the user.

\section{Conclusion}

We propose a bootstrap model averaging procedure to attenuate known
size distortions that arise when testing for the presence of a unit
root. We adopt a model-free bootstrap procedure where the null is
imposed by simple differencing, and we exploit recent developments in
automatic block length selection for the geometric bootstrap
procedure invoked. We adopt a model averaging procedure, and
simulations indicate that the proposed approach has acceptable size
and higher power than its peers (excluding those that exhibit
unacceptable size distortions such as the Phillips-Perron and ADF
tests). Since there are no nuisance parameters to be set by the user,
we are optimistic that the proposed approach will appeal to
practitioners.

\clearpage


\clearpage

\singlespacing

\bibliography{bmaur}

\end{document}
